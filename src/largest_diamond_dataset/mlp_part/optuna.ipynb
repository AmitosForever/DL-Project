{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4dc16d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "import optuna\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 50\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625f812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiamondPriceMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.3, use_batch_norm=True, target_mean=None):\n",
    "        super(DiamondPriceMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        self._initialize_weights(target_mean)\n",
    "    \n",
    "    def _initialize_weights(self, target_mean):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if target_mean is not None:\n",
    "            nn.init.constant_(self.network[-1].bias, target_mean)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e481964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory (for Jupyter Notebooks)\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (the \"father\" folder)\n",
    "parent_dir = os.path.dirname(script_dir)\n",
    "\n",
    "# Define the path to the preprocessed data directory relative to the script's location\n",
    "data_dir = os.path.join(parent_dir, 'data')\n",
    "\n",
    "# Define the path to the models directory relative to the parent directory of the script's location\n",
    "models_dir = os.path.join(parent_dir, 'models')\n",
    "\n",
    "# Create the models directory if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c7828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "X_train = np.load(f'{data_dir}/X_train_final.npy')\n",
    "X_val = np.load(f'{data_dir}/X_val_final.npy')\n",
    "X_test = np.load(f'{data_dir}/X_test_final.npy')\n",
    "\n",
    "y_train_log = np.load(f'{data_dir}/y_train_log.npy')  # Log-transformed version!\n",
    "y_val_log = np.load(f'{data_dir}/y_val_log.npy')\n",
    "y_test_log = np.load(f'{data_dir}/y_test_log.npy')\n",
    "\n",
    "def train_model(trial, X_train, y_train, X_val, y_val, device, \n",
    "                n_epochs=300, early_stopping_patience=40, early_stopping=False):\n",
    "    \"\"\"\n",
    "    Train model with hyperparameters suggested by Optuna\n",
    "    \n",
    "    Returns validation RMSE (lower is better)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # HYPERPARAMETERS TO TUNE\n",
    "    # ========================================================================\n",
    "    \n",
    "    # 1. Architecture\n",
    "    '''\n",
    "    n_layers = trial.suggest_int('n_layers', 3, 6)\n",
    "    hidden_dims = []\n",
    "    current_dim = trial.suggest_categorical('start_units', [1024, 256, 512])\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        # FIX: Suggest a 'shrink' decision (0 = same size, 1 = half size)\n",
    "        # The choices [0, 1] are now STATIC, so Optuna stays happy.\n",
    "        should_shrink = trial.suggest_categorical(f'shrink_layer_{i}', [0, 1])\n",
    "        \n",
    "        if should_shrink == 1 and current_dim > 64: # Safety floor\n",
    "            current_dim = current_dim // 2\n",
    "            \n",
    "        hidden_dims.append(current_dim)\n",
    "    '''\n",
    "    hidden_dims = [512, 256, 256, 128, 128, 64, 32]\n",
    "    \n",
    "    # 2. Regularization\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "    \n",
    "    # 3. Learning rate\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # 4. Batch size\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CREATE MODEL AND OPTIMIZER\n",
    "    # ========================================================================\n",
    "    \n",
    "    model = DiamondPriceMLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_batch_norm=use_batch_norm,\n",
    "        target_mean=np.mean(y_train_log)\n",
    "    ).to(device)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    train_criterion = nn.HuberLoss(delta=trial.suggest_float('huber_delta', 0.05, 1.0))\n",
    "    val_criterion = nn.MSELoss()\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_val),\n",
    "        torch.FloatTensor(y_val)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAINING LOOP\n",
    "    # ========================================================================\n",
    "    \n",
    "    best_val_mape = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for features, targets in train_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = train_criterion(output.view(-1), targets.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ← Gradient clipping\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_real_losses = []\n",
    "        val_mapes = []        # ← New list to store MAPE values\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                output = model(features)\n",
    "                loss = val_criterion(output.view(-1), targets.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                output_safe = torch.clamp(output, min=0, max=20)  # ← Clamping to avoid overflow\n",
    "                real_output = torch.expm1(output_safe)      # ← Changed from exp() to expm1()\n",
    "                real_targets = torch.expm1(targets)    # ← Changed from exp() to expm1()\n",
    "                real_loss = val_criterion(real_output.view(-1), real_targets.view(-1))\n",
    "                val_real_losses.append(real_loss.item())   \n",
    "\n",
    "                abs_percentage_error = torch.abs((real_targets - real_output) / (real_targets + 1e-8))\n",
    "                batch_mape = torch.mean(abs_percentage_error) * 100\n",
    "                val_mapes.append(batch_mape.item())  \n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_real_loss  = np.mean(val_real_losses)\n",
    "        val_real_rmse = np.sqrt(val_real_loss)\n",
    "        val_mape = np.mean(val_mapes)  # <--- Average MAPE for the epoch\n",
    "        \n",
    "        if val_mape < best_val_mape:\n",
    "                    best_val_mape = val_mape\n",
    "                    patience_counter = 0\n",
    "                    # Optional: Save best model state here\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(val_mape, epoch)\n",
    "        \n",
    "        if early_stopping and patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FINAL EVALUATION (Real RMSE)\n",
    "    # ========================================================================\n",
    "    \n",
    "    model.eval()\n",
    "    total_real_mse = 0.0\n",
    "    total_abs_perc_error = 0.0  # ← Added to track MAPE\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, targets in val_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # 1. Get model log-predictions\n",
    "            output_log = model(features).view(-1)\n",
    "            output_log_safe = torch.clamp(output_log, min=0, max=20)  # ← Clamping to avoid overflow\n",
    "            targets_log = targets.view(-1)\n",
    "            \n",
    "            # 2. Transform BOTH to real prices ($)\n",
    "            # We do this before the criterion to calculate RMSE in dollars\n",
    "            output_real = torch.expm1(output_log_safe)\n",
    "            targets_real = torch.expm1(targets_log)\n",
    "            \n",
    "            # 3. Calculate MSE & Absolute Percentage Error\n",
    "            batch_size = features.size(0)\n",
    "            \n",
    "            # MSE (for RMSE)\n",
    "            batch_mse = val_criterion(output_real, targets_real)\n",
    "            total_real_mse += batch_mse.item() * batch_size\n",
    "            \n",
    "            # MAPE: |(Actual - Predicted) / Actual|\n",
    "            # We use torch.sum here to get the total error for the batch\n",
    "            abs_perc_err = torch.abs((targets_real - output_real) / (targets_real + 1e-8))\n",
    "            total_abs_perc_error += torch.sum(abs_perc_err).item()\n",
    "            \n",
    "            # 4. Accumulate samples\n",
    "            total_samples += batch_size\n",
    "\n",
    "    # 5. Final RMSE calculation\n",
    "    final_real_mse = total_real_mse / total_samples\n",
    "    rmse = np.sqrt(final_real_mse)\n",
    "\n",
    "    # Calculate final MAPE as a percentage\n",
    "    mape = (total_abs_perc_error / total_samples) * 100\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b63eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to minimize\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"\\nTrial {trial.number}: Testing hyperparameters...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Train and evaluate\n",
    "    \n",
    "    mape = train_model(trial, X_train, y_train_log, X_val, y_val_log, device, early_stopping=False)\n",
    "    \n",
    "    # Check for invalid values\n",
    "    if np.isnan(mape) or np.isinf(mape):\n",
    "        rmse = 1000 # Large penalty\n",
    "    \n",
    "  \n",
    "    print(f\"  → Validation MAPE: ${mape:,.0f}\")\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98e1b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-17 14:42:35,284] A new study created in RDB with name: diamond-mlp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42fc89823634dd896b942de9e6eab55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 0: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 14:51:55,930] Trial 0 finished with value: 11.398057724438111 and parameters: {'dropout_rate': 0.08285785120200967, 'weight_decay': 1.2571392326591187e-06, 'use_batch_norm': True, 'learning_rate': 0.0012693276153465524, 'batch_size': 256, 'huber_delta': 0.48810830589323606}. Best is trial 0 with value: 11.398057724438111.\n",
      "\n",
      "Trial 1: Testing hyperparameters...\n",
      "  → Validation MAPE: $12\n",
      "[I 2026-01-17 14:58:09,624] Trial 1 finished with value: 12.197589788876213 and parameters: {'dropout_rate': 0.2644303418075645, 'weight_decay': 8.3781775950335e-05, 'use_batch_norm': True, 'learning_rate': 7.006693980604081e-05, 'batch_size': 512, 'huber_delta': 0.22477273362206307}. Best is trial 0 with value: 11.398057724438111.\n",
      "\n",
      "Trial 2: Testing hyperparameters...\n",
      "  → Validation MAPE: $15\n",
      "[I 2026-01-17 15:13:05,050] Trial 2 finished with value: 14.82023297378542 and parameters: {'dropout_rate': 0.3815380673483289, 'weight_decay': 8.177358802511865e-06, 'use_batch_norm': True, 'learning_rate': 0.0038357224246330277, 'batch_size': 128, 'huber_delta': 0.4566797303238432}. Best is trial 0 with value: 11.398057724438111.\n",
      "\n",
      "Trial 3: Testing hyperparameters...\n",
      "  → Validation MAPE: $15\n",
      "[I 2026-01-17 15:28:08,552] Trial 3 finished with value: 14.872521615490673 and parameters: {'dropout_rate': 0.32463804337281216, 'weight_decay': 0.00041144754296339577, 'use_batch_norm': True, 'learning_rate': 0.00042427222683252586, 'batch_size': 128, 'huber_delta': 0.6992140179043759}. Best is trial 0 with value: 11.398057724438111.\n",
      "\n",
      "Trial 4: Testing hyperparameters...\n",
      "  → Validation MAPE: $21\n",
      "[I 2026-01-17 15:39:47,198] Trial 4 finished with value: 20.58910064503221 and parameters: {'dropout_rate': 0.1840045709364458, 'weight_decay': 1.7219726537465797e-06, 'use_batch_norm': False, 'learning_rate': 0.009687439458193264, 'batch_size': 128, 'huber_delta': 0.855999410579923}. Best is trial 0 with value: 11.398057724438111.\n",
      "\n",
      "Trial 5: Testing hyperparameters...\n",
      "[I 2026-01-17 15:39:48,383] Trial 5 pruned. \n",
      "\n",
      "Trial 6: Testing hyperparameters...\n",
      "[I 2026-01-17 15:39:50,222] Trial 6 pruned. \n",
      "\n",
      "Trial 7: Testing hyperparameters...\n",
      "  → Validation MAPE: $12\n",
      "[I 2026-01-17 15:49:12,702] Trial 7 finished with value: 11.752333340121298 and parameters: {'dropout_rate': 0.21030888986366902, 'weight_decay': 1.1312452975479579e-06, 'use_batch_norm': True, 'learning_rate': 0.0012662614420441477, 'batch_size': 256, 'huber_delta': 0.8320373292558371}. Best is trial 0 with value: 11.398057724438111.\n",
      "\n",
      "Trial 8: Testing hyperparameters...\n",
      "[I 2026-01-17 15:49:13,926] Trial 8 pruned. \n",
      "\n",
      "Trial 9: Testing hyperparameters...\n",
      "[I 2026-01-17 15:49:17,175] Trial 9 pruned. \n",
      "\n",
      "Trial 10: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 15:56:56,034] Trial 10 finished with value: 11.264157672126867 and parameters: {'dropout_rate': 0.011920100195552147, 'weight_decay': 1.7106937282216607e-05, 'use_batch_norm': False, 'learning_rate': 0.0005700504541173073, 'batch_size': 256, 'huber_delta': 0.08456683199086856}. Best is trial 10 with value: 11.264157672126867.\n",
      "\n",
      "Trial 11: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 16:04:32,502] Trial 11 finished with value: 11.091126446902376 and parameters: {'dropout_rate': 0.01513836440619222, 'weight_decay': 1.735433816424705e-05, 'use_batch_norm': False, 'learning_rate': 0.0007464662688566814, 'batch_size': 256, 'huber_delta': 0.4112504417964262}. Best is trial 11 with value: 11.091126446902376.\n",
      "\n",
      "Trial 12: Testing hyperparameters...\n",
      "[I 2026-01-17 16:04:34,203] Trial 12 pruned. \n",
      "\n",
      "Trial 13: Testing hyperparameters...\n",
      "  → Validation MAPE: $10\n",
      "[I 2026-01-17 16:12:11,913] Trial 13 finished with value: 9.922148616899602 and parameters: {'dropout_rate': 0.0021759341366218397, 'weight_decay': 3.127616049041078e-05, 'use_batch_norm': False, 'learning_rate': 0.00013905679555526666, 'batch_size': 256, 'huber_delta': 0.08418299670478104}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 14: Testing hyperparameters...\n",
      "[I 2026-01-17 16:12:13,557] Trial 14 pruned. \n",
      "\n",
      "Trial 15: Testing hyperparameters...\n",
      "[I 2026-01-17 16:12:15,056] Trial 15 pruned. \n",
      "\n",
      "Trial 16: Testing hyperparameters...\n",
      "  → Validation MAPE: $10\n",
      "[I 2026-01-17 16:19:50,045] Trial 16 finished with value: 10.324910618397016 and parameters: {'dropout_rate': 0.004401615066544273, 'weight_decay': 5.308032005120256e-05, 'use_batch_norm': False, 'learning_rate': 0.00018117044813678072, 'batch_size': 256, 'huber_delta': 0.1965593857208128}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 17: Testing hyperparameters...\n",
      "[I 2026-01-17 16:19:51,733] Trial 17 pruned. \n",
      "\n",
      "Trial 18: Testing hyperparameters...\n",
      "[I 2026-01-17 16:19:53,241] Trial 18 pruned. \n",
      "\n",
      "Trial 19: Testing hyperparameters...\n",
      "[I 2026-01-17 16:19:54,425] Trial 19 pruned. \n",
      "\n",
      "Trial 20: Testing hyperparameters...\n",
      "[I 2026-01-17 16:19:55,925] Trial 20 pruned. \n",
      "\n",
      "Trial 21: Testing hyperparameters...\n",
      "[I 2026-01-17 16:20:00,497] Trial 21 pruned. \n",
      "\n",
      "Trial 22: Testing hyperparameters...\n",
      "[I 2026-01-17 16:20:01,996] Trial 22 pruned. \n",
      "\n",
      "Trial 23: Testing hyperparameters...\n",
      "[I 2026-01-17 16:20:03,636] Trial 23 pruned. \n",
      "\n",
      "Trial 24: Testing hyperparameters...\n",
      "[I 2026-01-17 16:20:05,131] Trial 24 pruned. \n",
      "\n",
      "Trial 25: Testing hyperparameters...\n",
      "  → Validation MAPE: $10\n",
      "[I 2026-01-17 16:27:40,673] Trial 25 finished with value: 10.44722553593326 and parameters: {'dropout_rate': 0.0007181310882202266, 'weight_decay': 1.260778656126692e-05, 'use_batch_norm': False, 'learning_rate': 0.0023598740980475376, 'batch_size': 256, 'huber_delta': 0.40576294850997463}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 26: Testing hyperparameters...\n",
      "[I 2026-01-17 16:27:42,330] Trial 26 pruned. \n",
      "\n",
      "Trial 27: Testing hyperparameters...\n",
      "[I 2026-01-17 16:27:43,825] Trial 27 pruned. \n",
      "\n",
      "Trial 28: Testing hyperparameters...\n",
      "[I 2026-01-17 16:27:46,163] Trial 28 pruned. \n",
      "\n",
      "Trial 29: Testing hyperparameters...\n",
      "[I 2026-01-17 16:27:47,342] Trial 29 pruned. \n",
      "\n",
      "Trial 30: Testing hyperparameters...\n",
      "[I 2026-01-17 16:27:48,838] Trial 30 pruned. \n",
      "\n",
      "Trial 31: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 16:35:22,378] Trial 31 finished with value: 10.56744646875891 and parameters: {'dropout_rate': 0.0024864743816812194, 'weight_decay': 1.591307422890838e-05, 'use_batch_norm': False, 'learning_rate': 0.0009873878952576633, 'batch_size': 256, 'huber_delta': 0.43297412421757203}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 32: Testing hyperparameters...\n",
      "[I 2026-01-17 16:35:24,027] Trial 32 pruned. \n",
      "\n",
      "Trial 33: Testing hyperparameters...\n",
      "[I 2026-01-17 16:35:25,595] Trial 33 pruned. \n",
      "\n",
      "Trial 34: Testing hyperparameters...\n",
      "[I 2026-01-17 16:35:27,112] Trial 34 pruned. \n",
      "\n",
      "Trial 35: Testing hyperparameters...\n",
      "[I 2026-01-17 16:35:29,470] Trial 35 pruned. \n",
      "\n",
      "Trial 36: Testing hyperparameters...\n",
      "[I 2026-01-17 16:35:30,839] Trial 36 pruned. \n",
      "\n",
      "Trial 37: Testing hyperparameters...\n",
      "[I 2026-01-17 16:35:32,364] Trial 37 pruned. \n",
      "\n",
      "Trial 38: Testing hyperparameters...\n",
      "[I 2026-01-17 16:35:35,415] Trial 38 pruned. \n",
      "\n",
      "Trial 39: Testing hyperparameters...\n",
      "  → Validation MAPE: $12\n",
      "[I 2026-01-17 16:43:12,461] Trial 39 finished with value: 11.511118288053117 and parameters: {'dropout_rate': 0.0038554634096091586, 'weight_decay': 8.105682741231378e-06, 'use_batch_norm': False, 'learning_rate': 0.009751552723874266, 'batch_size': 256, 'huber_delta': 0.19406561951088114}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 40: Testing hyperparameters...\n",
      "[I 2026-01-17 16:43:13,672] Trial 40 pruned. \n",
      "\n",
      "Trial 41: Testing hyperparameters...\n",
      "[I 2026-01-17 16:43:15,171] Trial 41 pruned. \n",
      "\n",
      "Trial 42: Testing hyperparameters...\n",
      "[I 2026-01-17 16:43:16,822] Trial 42 pruned. \n",
      "\n",
      "Trial 43: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 16:50:55,865] Trial 43 finished with value: 11.189057925048692 and parameters: {'dropout_rate': 0.0005986923129051528, 'weight_decay': 8.25626194439175e-06, 'use_batch_norm': False, 'learning_rate': 0.0005183995388503481, 'batch_size': 256, 'huber_delta': 0.49907950439693766}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 44: Testing hyperparameters...\n",
      "[I 2026-01-17 16:51:00,477] Trial 44 pruned. \n",
      "\n",
      "Trial 45: Testing hyperparameters...\n",
      "[I 2026-01-17 16:51:02,144] Trial 45 pruned. \n",
      "\n",
      "Trial 46: Testing hyperparameters...\n",
      "[I 2026-01-17 16:51:03,661] Trial 46 pruned. \n",
      "\n",
      "Trial 47: Testing hyperparameters...\n",
      "[I 2026-01-17 16:51:06,724] Trial 47 pruned. \n",
      "\n",
      "Trial 48: Testing hyperparameters...\n",
      "[I 2026-01-17 16:51:09,869] Trial 48 pruned. \n",
      "\n",
      "Trial 49: Testing hyperparameters...\n",
      "[I 2026-01-17 16:51:11,384] Trial 49 pruned. \n",
      "\n",
      "Trial 50: Testing hyperparameters...\n",
      "[I 2026-01-17 16:51:12,895] Trial 50 pruned. \n",
      "\n",
      "Trial 51: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 16:58:48,705] Trial 51 finished with value: 11.098618270056152 and parameters: {'dropout_rate': 0.004079666058045105, 'weight_decay': 7.347160128918428e-06, 'use_batch_norm': False, 'learning_rate': 0.0005316803430889077, 'batch_size': 256, 'huber_delta': 0.5172342275772758}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 52: Testing hyperparameters...\n",
      "[I 2026-01-17 16:58:51,802] Trial 52 pruned. \n",
      "\n",
      "Trial 53: Testing hyperparameters...\n",
      "[I 2026-01-17 16:58:53,298] Trial 53 pruned. \n",
      "\n",
      "Trial 54: Testing hyperparameters...\n",
      "[I 2026-01-17 16:58:54,791] Trial 54 pruned. \n",
      "\n",
      "Trial 55: Testing hyperparameters...\n",
      "[I 2026-01-17 16:58:56,431] Trial 55 pruned. \n",
      "\n",
      "Trial 56: Testing hyperparameters...\n",
      "[I 2026-01-17 16:58:57,456] Trial 56 pruned. \n",
      "\n",
      "Trial 57: Testing hyperparameters...\n",
      "[I 2026-01-17 16:58:59,121] Trial 57 pruned. \n",
      "\n",
      "Trial 58: Testing hyperparameters...\n",
      "[I 2026-01-17 16:59:00,964] Trial 58 pruned. \n",
      "\n",
      "Trial 59: Testing hyperparameters...\n",
      "[I 2026-01-17 16:59:02,468] Trial 59 pruned. \n",
      "\n",
      "Trial 60: Testing hyperparameters...\n",
      "[I 2026-01-17 16:59:04,802] Trial 60 pruned. \n",
      "\n",
      "Trial 61: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 17:06:39,947] Trial 61 finished with value: 11.131455740163661 and parameters: {'dropout_rate': 0.0018359887281153946, 'weight_decay': 3.143094752289253e-05, 'use_batch_norm': False, 'learning_rate': 0.0005023154777023826, 'batch_size': 256, 'huber_delta': 0.5020111462571686}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 62: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:44,560] Trial 62 pruned. \n",
      "\n",
      "Trial 63: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:46,072] Trial 63 pruned. \n",
      "\n",
      "Trial 64: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:47,740] Trial 64 pruned. \n",
      "\n",
      "Trial 65: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:49,249] Trial 65 pruned. \n",
      "\n",
      "Trial 66: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:50,760] Trial 66 pruned. \n",
      "\n",
      "Trial 67: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:52,423] Trial 67 pruned. \n",
      "\n",
      "Trial 68: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:53,938] Trial 68 pruned. \n",
      "\n",
      "Trial 69: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:55,128] Trial 69 pruned. \n",
      "\n",
      "Trial 70: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:56,981] Trial 70 pruned. \n",
      "\n",
      "Trial 71: Testing hyperparameters...\n",
      "[I 2026-01-17 17:06:58,492] Trial 71 pruned. \n",
      "\n",
      "Trial 72: Testing hyperparameters...\n",
      "[I 2026-01-17 17:07:00,156] Trial 72 pruned. \n",
      "\n",
      "Trial 73: Testing hyperparameters...\n",
      "[I 2026-01-17 17:07:03,134] Trial 73 pruned. \n",
      "\n",
      "Trial 74: Testing hyperparameters...\n",
      "[I 2026-01-17 17:07:04,800] Trial 74 pruned. \n",
      "\n",
      "Trial 75: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 17:14:39,359] Trial 75 finished with value: 11.178706156643612 and parameters: {'dropout_rate': 0.0012852620847754422, 'weight_decay': 1.693402833366037e-05, 'use_batch_norm': False, 'learning_rate': 0.00021875882766555124, 'batch_size': 256, 'huber_delta': 0.42214340325515815}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 76: Testing hyperparameters...\n",
      "[I 2026-01-17 17:14:41,020] Trial 76 pruned. \n",
      "\n",
      "Trial 77: Testing hyperparameters...\n",
      "[I 2026-01-17 17:14:43,350] Trial 77 pruned. \n",
      "\n",
      "Trial 78: Testing hyperparameters...\n",
      "[I 2026-01-17 17:14:44,842] Trial 78 pruned. \n",
      "\n",
      "Trial 79: Testing hyperparameters...\n",
      "[I 2026-01-17 17:14:46,339] Trial 79 pruned. \n",
      "\n",
      "Trial 80: Testing hyperparameters...\n",
      "[I 2026-01-17 17:14:48,024] Trial 80 pruned. \n",
      "\n",
      "Trial 81: Testing hyperparameters...\n",
      "[I 2026-01-17 17:15:00,324] Trial 81 pruned. \n",
      "\n",
      "Trial 82: Testing hyperparameters...\n",
      "[I 2026-01-17 17:15:01,970] Trial 82 pruned. \n",
      "\n",
      "Trial 83: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 17:22:37,346] Trial 83 finished with value: 11.425614583723425 and parameters: {'dropout_rate': 0.011458930207021762, 'weight_decay': 2.3805630928518377e-05, 'use_batch_norm': False, 'learning_rate': 0.0008562122817434649, 'batch_size': 256, 'huber_delta': 0.4952911611380911}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 84: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 17:30:13,757] Trial 84 finished with value: 11.079482830820723 and parameters: {'dropout_rate': 0.002074869571179587, 'weight_decay': 9.45850530674302e-06, 'use_batch_norm': False, 'learning_rate': 0.0005596207161749788, 'batch_size': 256, 'huber_delta': 0.4061245088730596}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 85: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:15,268] Trial 85 pruned. \n",
      "\n",
      "Trial 86: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:16,446] Trial 86 pruned. \n",
      "\n",
      "Trial 87: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:25,596] Trial 87 pruned. \n",
      "\n",
      "Trial 88: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:27,234] Trial 88 pruned. \n",
      "\n",
      "Trial 89: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:28,728] Trial 89 pruned. \n",
      "\n",
      "Trial 90: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:31,083] Trial 90 pruned. \n",
      "\n",
      "Trial 91: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:41,593] Trial 91 pruned. \n",
      "\n",
      "Trial 92: Testing hyperparameters...\n",
      "[I 2026-01-17 17:30:47,743] Trial 92 pruned. \n",
      "\n",
      "Trial 93: Testing hyperparameters...\n",
      "  → Validation MAPE: $11\n",
      "[I 2026-01-17 17:38:26,618] Trial 93 finished with value: 11.348202517993423 and parameters: {'dropout_rate': 6.639271305801321e-05, 'weight_decay': 8.290670277230785e-06, 'use_batch_norm': False, 'learning_rate': 0.0004205094935187279, 'batch_size': 256, 'huber_delta': 0.3620157485286355}. Best is trial 13 with value: 9.922148616899602.\n",
      "\n",
      "Trial 94: Testing hyperparameters...\n",
      "[I 2026-01-17 17:38:28,133] Trial 94 pruned. \n",
      "\n",
      "Trial 95: Testing hyperparameters...\n",
      "[I 2026-01-17 17:38:29,800] Trial 95 pruned. \n",
      "\n",
      "Trial 96: Testing hyperparameters...\n",
      "[I 2026-01-17 17:38:31,311] Trial 96 pruned. \n",
      "\n",
      "Trial 97: Testing hyperparameters...\n",
      "[I 2026-01-17 17:38:32,819] Trial 97 pruned. \n",
      "\n",
      "Trial 98: Testing hyperparameters...\n",
      "[I 2026-01-17 17:38:34,487] Trial 98 pruned. \n",
      "\n",
      "Trial 99: Testing hyperparameters...\n",
      "[I 2026-01-17 17:38:36,351] Trial 99 pruned. \n",
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  80\n",
      "  Number of complete trials:  20\n",
      "Best trial:\n",
      "  Value:  9.922148616899602\n",
      "  Params: \n",
      "    dropout_rate: 0.0021759341366218397\n",
      "    weight_decay: 3.127616049041078e-05\n",
      "    use_batch_norm: False\n",
      "    learning_rate: 0.00013905679555526666\n",
      "    batch_size: 256\n",
      "    huber_delta: 0.08418299670478104\n"
     ]
    }
   ],
   "source": [
    "# now we can run the experiment\n",
    "storage_name = \"sqlite:///diamond_study.db\"\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"diamond-mlp\", \n",
    "    storage=storage_name, \n",
    "    direction=\"minimize\",\n",
    "    load_if_exists=True  # Crucial for restarting\n",
    ")\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True, catch=(Exception,))\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
