{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4dc16d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "import optuna\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 50\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625f812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiamondPriceMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.3, use_batch_norm=True, target_mean=None):\n",
    "        super(DiamondPriceMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        self._initialize_weights(target_mean)\n",
    "    \n",
    "    def _initialize_weights(self, target_mean):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if target_mean is not None:\n",
    "            nn.init.constant_(self.network[-1].bias, target_mean)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c7828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/X_train_final.npy')\n",
    "y_train_log = np.load('data/y_train_log.npy')\n",
    "X_val = np.load('data/X_val_final.npy')\n",
    "y_val_log = np.load('data/y_val_log.npy')\n",
    "\n",
    "def train_model(trial, X_train, y_train, X_val, y_val, device, \n",
    "                n_epochs=300, early_stopping_patience=40, early_stopping=False):\n",
    "    \"\"\"\n",
    "    Train model with hyperparameters suggested by Optuna\n",
    "    \n",
    "    Returns validation RMSE (lower is better)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # HYPERPARAMETERS TO TUNE\n",
    "    # ========================================================================\n",
    "    \n",
    "    # 1. Architecture\n",
    "    '''\n",
    "    n_layers = trial.suggest_int('n_layers', 3, 6)\n",
    "    hidden_dims = []\n",
    "    current_dim = trial.suggest_categorical('start_units', [1024, 256, 512])\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        # FIX: Suggest a 'shrink' decision (0 = same size, 1 = half size)\n",
    "        # The choices [0, 1] are now STATIC, so Optuna stays happy.\n",
    "        should_shrink = trial.suggest_categorical(f'shrink_layer_{i}', [0, 1])\n",
    "        \n",
    "        if should_shrink == 1 and current_dim > 64: # Safety floor\n",
    "            current_dim = current_dim // 2\n",
    "            \n",
    "        hidden_dims.append(current_dim)\n",
    "    '''\n",
    "    hidden_dims = [512, 256, 256, 128, 128, 64, 32]\n",
    "    \n",
    "    # 2. Regularization\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "    \n",
    "    # 3. Learning rate\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # 4. Batch size\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CREATE MODEL AND OPTIMIZER\n",
    "    # ========================================================================\n",
    "    \n",
    "    model = DiamondPriceMLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_batch_norm=use_batch_norm,\n",
    "        target_mean=np.mean(y_train_log)\n",
    "    ).to(device)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    train_criterion = nn.HuberLoss(delta=trial.suggest_float('huber_delta', 0.05, 1.0))\n",
    "    val_criterion = nn.MSELoss()\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_val),\n",
    "        torch.FloatTensor(y_val)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAINING LOOP\n",
    "    # ========================================================================\n",
    "    \n",
    "    best_val_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for features, targets in train_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = train_criterion(output.view(-1), targets.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ← Gradient clipping\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_real_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                output = model(features)\n",
    "                loss = val_criterion(output.view(-1), targets.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                output_safe = torch.clamp(output, min=0, max=20)  # ← Clamping to avoid overflow\n",
    "                real_output = torch.expm1(output_safe)      # ← Changed from exp() to expm1()\n",
    "                real_targets = torch.expm1(targets)    # ← Changed from exp() to expm1()\n",
    "                real_loss = val_criterion(real_output.view(-1), real_targets.view(-1))\n",
    "                val_real_losses.append(real_loss.item())   \n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_real_loss  = np.mean(val_real_losses)\n",
    "        val_real_rmse = np.sqrt(val_real_loss)\n",
    "        \n",
    "        if val_real_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_real_rmse\n",
    "                    patience_counter = 0\n",
    "                    # Optional: Save best model state here\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(val_real_rmse, epoch)\n",
    "        \n",
    "        if early_stopping and patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FINAL EVALUATION (Real RMSE)\n",
    "    # ========================================================================\n",
    "    \n",
    "    model.eval()\n",
    "    total_real_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, targets in val_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # 1. Get model log-predictions\n",
    "            output_log = model(features).view(-1)\n",
    "            output_log_safe = torch.clamp(output_log, min=0, max=20)  # ← Clamping to avoid overflow\n",
    "            targets_log = targets.view(-1)\n",
    "            \n",
    "            # 2. Transform BOTH to real prices ($)\n",
    "            # We do this before the criterion to calculate RMSE in dollars\n",
    "            output_real = torch.expm1(output_log_safe)\n",
    "            targets_real = torch.expm1(targets_log)\n",
    "            \n",
    "            # 3. Calculate MSE for this batch in real dollars\n",
    "            # val_criterion is nn.MSELoss()\n",
    "            batch_mse = val_criterion(output_real, targets_real)\n",
    "            \n",
    "            # 4. Accumulate weighted by batch size (handles smaller last batches)\n",
    "            batch_size = features.size(0)\n",
    "            total_real_mse += batch_mse.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    # 5. Final RMSE calculation\n",
    "    final_real_mse = total_real_mse / total_samples\n",
    "    rmse = np.sqrt(final_real_mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b63eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to minimize\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"\\nTrial {trial.number}: Testing hyperparameters...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Train and evaluate\n",
    "    \n",
    "    rmse = train_model(trial, X_train, y_train_log, X_val, y_val_log, device, early_stopping=False)\n",
    "    \n",
    "    # Check for invalid values\n",
    "    if np.isnan(rmse) or np.isinf(rmse):\n",
    "        rmse = 1e10 # Large penalty\n",
    "    \n",
    "  \n",
    "    print(f\"  → Validation RMSE: ${rmse:,.0f}\")\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98e1b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-10 13:24:37,957] A new study created in RDB with name: diamond-mlp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a607e8ac6bbd4f5d88e9f65397b36e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 0: Testing hyperparameters...\n",
      "  → Validation RMSE: $14,038\n",
      "[I 2026-01-10 13:34:57,878] Trial 0 finished with value: 14038.198408047916 and parameters: {'dropout_rate': 0.4835703256465729, 'weight_decay': 1.0031460941124857e-06, 'use_batch_norm': True, 'learning_rate': 0.0004389636493392904, 'batch_size': 512, 'huber_delta': 0.2238223925112907}. Best is trial 0 with value: 14038.198408047916.\n",
      "\n",
      "Trial 1: Testing hyperparameters...\n",
      "  → Validation RMSE: $21,808\n",
      "[I 2026-01-10 13:55:39,997] Trial 1 finished with value: 21808.019355531163 and parameters: {'dropout_rate': 0.31319016592441107, 'weight_decay': 2.0119786395854447e-06, 'use_batch_norm': False, 'learning_rate': 0.0038449571088975606, 'batch_size': 256, 'huber_delta': 0.7148217317548615}. Best is trial 0 with value: 14038.198408047916.\n",
      "\n",
      "Trial 2: Testing hyperparameters...\n",
      "  → Validation RMSE: $14,606\n",
      "[I 2026-01-10 14:05:57,467] Trial 2 finished with value: 14605.811523484776 and parameters: {'dropout_rate': 0.18197955320978676, 'weight_decay': 3.339031377132154e-05, 'use_batch_norm': True, 'learning_rate': 2.2196268530782126e-05, 'batch_size': 512, 'huber_delta': 0.5105314227131106}. Best is trial 0 with value: 14038.198408047916.\n",
      "\n",
      "Trial 3: Testing hyperparameters...\n",
      "  → Validation RMSE: $20,822\n",
      "[I 2026-01-10 14:15:59,483] Trial 3 finished with value: 20821.92801900643 and parameters: {'dropout_rate': 0.20979472188552833, 'weight_decay': 8.133761367572995e-06, 'use_batch_norm': False, 'learning_rate': 0.0014591696822572016, 'batch_size': 512, 'huber_delta': 0.5069859981009717}. Best is trial 0 with value: 14038.198408047916.\n",
      "\n",
      "Trial 4: Testing hyperparameters...\n",
      "  → Validation RMSE: $13,455\n",
      "[I 2026-01-10 14:42:01,301] Trial 4 finished with value: 13454.771969505542 and parameters: {'dropout_rate': 0.2885499580187689, 'weight_decay': 1.2874592065442472e-06, 'use_batch_norm': True, 'learning_rate': 8.26912709222637e-05, 'batch_size': 256, 'huber_delta': 0.13074543723606946}. Best is trial 4 with value: 13454.771969505542.\n",
      "\n",
      "Trial 5: Testing hyperparameters...\n",
      "[I 2026-01-10 14:42:05,514] Trial 5 pruned. \n",
      "\n",
      "Trial 6: Testing hyperparameters...\n",
      "[I 2026-01-10 14:42:21,092] Trial 6 pruned. \n",
      "\n",
      "Trial 7: Testing hyperparameters...\n",
      "[I 2026-01-10 14:42:37,607] Trial 7 pruned. \n",
      "\n",
      "Trial 8: Testing hyperparameters...\n",
      "[I 2026-01-10 14:42:39,737] Trial 8 pruned. \n",
      "\n",
      "Trial 9: Testing hyperparameters...\n",
      "[I 2026-01-10 14:42:55,311] Trial 9 pruned. \n",
      "\n",
      "Trial 10: Testing hyperparameters...\n",
      "[I 2026-01-10 14:43:00,736] Trial 10 pruned. \n",
      "\n",
      "Trial 11: Testing hyperparameters...\n",
      "[I 2026-01-10 14:49:09,259] Trial 11 pruned. \n",
      "\n",
      "Trial 12: Testing hyperparameters...\n",
      "  → Validation RMSE: $17,104\n",
      "[I 2026-01-10 14:59:28,347] Trial 12 finished with value: 17104.17686193805 and parameters: {'dropout_rate': 0.4982443165730731, 'weight_decay': 6.389299066209821e-06, 'use_batch_norm': True, 'learning_rate': 0.0004293202631013414, 'batch_size': 512, 'huber_delta': 0.06630368132742058}. Best is trial 4 with value: 13454.771969505542.\n",
      "\n",
      "Trial 13: Testing hyperparameters...\n",
      "[I 2026-01-10 14:59:30,437] Trial 13 pruned. \n",
      "\n",
      "Trial 14: Testing hyperparameters...\n",
      "[I 2026-01-10 14:59:35,624] Trial 14 pruned. \n",
      "\n",
      "Trial 15: Testing hyperparameters...\n",
      "[I 2026-01-10 14:59:41,203] Trial 15 pruned. \n",
      "\n",
      "Trial 16: Testing hyperparameters...\n",
      "[I 2026-01-10 14:59:43,355] Trial 16 pruned. \n",
      "\n",
      "Trial 17: Testing hyperparameters...\n",
      "[I 2026-01-10 14:59:48,801] Trial 17 pruned. \n",
      "\n",
      "Trial 18: Testing hyperparameters...\n",
      "[I 2026-01-10 14:59:50,892] Trial 18 pruned. \n",
      "\n",
      "Trial 19: Testing hyperparameters...\n",
      "[I 2026-01-10 14:59:59,349] Trial 19 pruned. \n",
      "\n",
      "Trial 20: Testing hyperparameters...\n",
      "[I 2026-01-10 15:00:01,417] Trial 20 pruned. \n",
      "\n",
      "Trial 21: Testing hyperparameters...\n",
      "[I 2026-01-10 15:00:03,728] Trial 21 pruned. \n",
      "\n",
      "Trial 22: Testing hyperparameters...\n",
      "  → Validation RMSE: $16,273\n",
      "[I 2026-01-10 15:10:23,794] Trial 22 finished with value: 16272.974600060383 and parameters: {'dropout_rate': 0.08292473470066464, 'weight_decay': 0.00015864287008605015, 'use_batch_norm': True, 'learning_rate': 4.021602237213543e-05, 'batch_size': 512, 'huber_delta': 0.1637028473083541}. Best is trial 4 with value: 13454.771969505542.\n",
      "\n",
      "Trial 23: Testing hyperparameters...\n",
      "[I 2026-01-10 15:12:43,736] Trial 23 pruned. \n",
      "\n",
      "Trial 24: Testing hyperparameters...\n",
      "[I 2026-01-10 15:12:46,006] Trial 24 pruned. \n",
      "\n",
      "Trial 25: Testing hyperparameters...\n",
      "[I 2026-01-10 15:12:51,096] Trial 25 pruned. \n",
      "\n",
      "Trial 26: Testing hyperparameters...\n",
      "[I 2026-01-10 15:12:53,069] Trial 26 pruned. \n",
      "\n",
      "Trial 27: Testing hyperparameters...\n",
      "  → Validation RMSE: $10,450\n",
      "[I 2026-01-10 15:23:11,235] Trial 27 finished with value: 10450.308026881172 and parameters: {'dropout_rate': 0.0129711958314368, 'weight_decay': 1.1462044111454698e-05, 'use_batch_norm': True, 'learning_rate': 0.00019003894987789028, 'batch_size': 512, 'huber_delta': 0.981062295939513}. Best is trial 27 with value: 10450.308026881172.\n",
      "\n",
      "Trial 28: Testing hyperparameters...\n",
      "[I 2026-01-10 15:23:19,509] Trial 28 pruned. \n",
      "\n",
      "Trial 29: Testing hyperparameters...\n",
      "[I 2026-01-10 15:23:23,648] Trial 29 pruned. \n",
      "\n",
      "Trial 30: Testing hyperparameters...\n",
      "[I 2026-01-10 15:23:29,036] Trial 30 pruned. \n",
      "\n",
      "Trial 31: Testing hyperparameters...\n",
      "[I 2026-01-10 15:23:31,361] Trial 31 pruned. \n",
      "\n",
      "Trial 32: Testing hyperparameters...\n",
      "[I 2026-01-10 15:23:33,521] Trial 32 pruned. \n",
      "\n",
      "Trial 33: Testing hyperparameters...\n",
      "  → Validation RMSE: $45,491\n",
      "[I 2026-01-10 15:33:52,906] Trial 33 finished with value: 45490.537526964865 and parameters: {'dropout_rate': 0.03988667343676326, 'weight_decay': 0.0001094429155017159, 'use_batch_norm': True, 'learning_rate': 0.0022832759140434798, 'batch_size': 512, 'huber_delta': 0.8667291198502316}. Best is trial 27 with value: 10450.308026881172.\n",
      "\n",
      "Trial 34: Testing hyperparameters...\n",
      "[I 2026-01-10 15:33:55,060] Trial 34 pruned. \n",
      "\n",
      "Trial 35: Testing hyperparameters...\n",
      "[I 2026-01-10 15:33:57,249] Trial 35 pruned. \n",
      "\n",
      "Trial 36: Testing hyperparameters...\n",
      "[I 2026-01-10 15:33:59,419] Trial 36 pruned. \n",
      "\n",
      "Trial 37: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:07,812] Trial 37 pruned. \n",
      "\n",
      "Trial 38: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:13,069] Trial 38 pruned. \n",
      "\n",
      "Trial 39: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:15,083] Trial 39 pruned. \n",
      "\n",
      "Trial 40: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:23,325] Trial 40 pruned. \n",
      "\n",
      "Trial 41: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:25,629] Trial 41 pruned. \n",
      "\n",
      "Trial 42: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:31,902] Trial 42 pruned. \n",
      "\n",
      "Trial 43: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:34,061] Trial 43 pruned. \n",
      "\n",
      "Trial 44: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:36,383] Trial 44 pruned. \n",
      "\n",
      "Trial 45: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:41,761] Trial 45 pruned. \n",
      "\n",
      "Trial 46: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:44,123] Trial 46 pruned. \n",
      "\n",
      "Trial 47: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:46,232] Trial 47 pruned. \n",
      "\n",
      "Trial 48: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:51,470] Trial 48 pruned. \n",
      "\n",
      "Trial 49: Testing hyperparameters...\n",
      "[I 2026-01-10 15:34:53,640] Trial 49 pruned. \n",
      "\n",
      "Trial 50: Testing hyperparameters...\n",
      "[I 2026-01-10 15:39:00,926] Trial 50 pruned. \n",
      "\n",
      "Trial 51: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:07,044] Trial 51 pruned. \n",
      "\n",
      "Trial 52: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:09,204] Trial 52 pruned. \n",
      "\n",
      "Trial 53: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:11,332] Trial 53 pruned. \n",
      "\n",
      "Trial 54: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:13,469] Trial 54 pruned. \n",
      "\n",
      "Trial 55: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:15,774] Trial 55 pruned. \n",
      "\n",
      "Trial 56: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:21,103] Trial 56 pruned. \n",
      "\n",
      "Trial 57: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:23,422] Trial 57 pruned. \n",
      "\n",
      "Trial 58: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:25,566] Trial 58 pruned. \n",
      "\n",
      "Trial 59: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:30,842] Trial 59 pruned. \n",
      "\n",
      "Trial 60: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:32,918] Trial 60 pruned. \n",
      "\n",
      "Trial 61: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:35,084] Trial 61 pruned. \n",
      "\n",
      "Trial 62: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:37,081] Trial 62 pruned. \n",
      "\n",
      "Trial 63: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:39,267] Trial 63 pruned. \n",
      "\n",
      "Trial 64: Testing hyperparameters...\n",
      "[I 2026-01-10 15:40:41,254] Trial 64 pruned. \n",
      "\n",
      "Trial 65: Testing hyperparameters...\n",
      "[I 2026-01-10 15:44:23,709] Trial 65 pruned. \n",
      "\n",
      "Trial 66: Testing hyperparameters...\n",
      "[I 2026-01-10 15:44:29,258] Trial 66 pruned. \n",
      "\n",
      "Trial 67: Testing hyperparameters...\n",
      "[I 2026-01-10 15:44:37,582] Trial 67 pruned. \n",
      "\n",
      "Trial 68: Testing hyperparameters...\n",
      "[I 2026-01-10 15:44:39,584] Trial 68 pruned. \n",
      "\n",
      "Trial 69: Testing hyperparameters...\n",
      "[I 2026-01-10 15:44:41,639] Trial 69 pruned. \n",
      "\n",
      "Trial 70: Testing hyperparameters...\n",
      "[I 2026-01-10 15:47:32,158] Trial 70 pruned. \n",
      "\n",
      "Trial 71: Testing hyperparameters...\n",
      "[I 2026-01-10 15:53:45,263] Trial 71 pruned. \n",
      "\n",
      "Trial 72: Testing hyperparameters...\n",
      "[I 2026-01-10 15:59:56,961] Trial 72 pruned. \n",
      "\n",
      "Trial 73: Testing hyperparameters...\n",
      "[I 2026-01-10 16:06:08,178] Trial 73 pruned. \n",
      "\n",
      "Trial 74: Testing hyperparameters...\n",
      "[I 2026-01-10 16:12:21,198] Trial 74 pruned. \n",
      "\n",
      "Trial 75: Testing hyperparameters...\n",
      "[I 2026-01-10 16:13:25,515] Trial 75 pruned. \n",
      "\n",
      "Trial 76: Testing hyperparameters...\n",
      "[I 2026-01-10 16:13:34,020] Trial 76 pruned. \n",
      "\n",
      "Trial 77: Testing hyperparameters...\n",
      "[I 2026-01-10 16:13:36,162] Trial 77 pruned. \n",
      "\n",
      "Trial 78: Testing hyperparameters...\n",
      "[I 2026-01-10 16:13:40,847] Trial 78 pruned. \n",
      "\n",
      "Trial 79: Testing hyperparameters...\n",
      "[I 2026-01-10 16:13:43,029] Trial 79 pruned. \n",
      "\n",
      "Trial 80: Testing hyperparameters...\n",
      "[I 2026-01-10 16:13:45,350] Trial 80 pruned. \n",
      "\n",
      "Trial 81: Testing hyperparameters...\n",
      "[I 2026-01-10 16:13:47,495] Trial 81 pruned. \n",
      "\n",
      "Trial 82: Testing hyperparameters...\n",
      "  → Validation RMSE: $15,101\n",
      "[I 2026-01-10 16:24:09,242] Trial 82 finished with value: 15101.490854596137 and parameters: {'dropout_rate': 0.015382097632728289, 'weight_decay': 0.00016945761641564677, 'use_batch_norm': True, 'learning_rate': 0.0022288333772765637, 'batch_size': 512, 'huber_delta': 0.8906434804061205}. Best is trial 27 with value: 10450.308026881172.\n",
      "\n",
      "Trial 83: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:11,529] Trial 83 pruned. \n",
      "\n",
      "Trial 84: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:17,930] Trial 84 pruned. \n",
      "\n",
      "Trial 85: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:20,063] Trial 85 pruned. \n",
      "\n",
      "Trial 86: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:22,371] Trial 86 pruned. \n",
      "\n",
      "Trial 87: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:27,585] Trial 87 pruned. \n",
      "\n",
      "Trial 88: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:29,863] Trial 88 pruned. \n",
      "\n",
      "Trial 89: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:31,995] Trial 89 pruned. \n",
      "\n",
      "Trial 90: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:40,283] Trial 90 pruned. \n",
      "\n",
      "Trial 91: Testing hyperparameters...\n",
      "[I 2026-01-10 16:24:42,306] Trial 91 pruned. \n",
      "\n",
      "Trial 92: Testing hyperparameters...\n",
      "  → Validation RMSE: $18,255\n",
      "[I 2026-01-10 16:35:02,396] Trial 92 finished with value: 18254.63832419872 and parameters: {'dropout_rate': 0.02712344075081754, 'weight_decay': 0.00010905196190770966, 'use_batch_norm': True, 'learning_rate': 0.0032422391838589013, 'batch_size': 512, 'huber_delta': 0.9064128311812196}. Best is trial 27 with value: 10450.308026881172.\n",
      "\n",
      "Trial 93: Testing hyperparameters...\n",
      "  → Validation RMSE: $24,205\n",
      "[I 2026-01-10 16:45:21,458] Trial 93 finished with value: 24204.987414946005 and parameters: {'dropout_rate': 0.01644112148218657, 'weight_decay': 0.00011960270738742187, 'use_batch_norm': True, 'learning_rate': 0.0035065314998954057, 'batch_size': 512, 'huber_delta': 0.9030153767674495}. Best is trial 27 with value: 10450.308026881172.\n",
      "\n",
      "Trial 94: Testing hyperparameters...\n",
      "[I 2026-01-10 16:45:23,793] Trial 94 pruned. \n",
      "\n",
      "Trial 95: Testing hyperparameters...\n",
      "[I 2026-01-10 16:45:25,939] Trial 95 pruned. \n",
      "\n",
      "Trial 96: Testing hyperparameters...\n",
      "[I 2026-01-10 16:45:40,448] Trial 96 pruned. \n",
      "\n",
      "Trial 97: Testing hyperparameters...\n",
      "[I 2026-01-10 16:45:42,462] Trial 97 pruned. \n",
      "\n",
      "Trial 98: Testing hyperparameters...\n",
      "[I 2026-01-10 16:45:47,721] Trial 98 pruned. \n",
      "\n",
      "Trial 99: Testing hyperparameters...\n",
      "[I 2026-01-10 16:45:50,016] Trial 99 pruned. \n",
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  88\n",
      "  Number of complete trials:  12\n",
      "Best trial:\n",
      "  Value:  10450.308026881172\n",
      "  Params: \n",
      "    dropout_rate: 0.0129711958314368\n",
      "    weight_decay: 1.1462044111454698e-05\n",
      "    use_batch_norm: True\n",
      "    learning_rate: 0.00019003894987789028\n",
      "    batch_size: 512\n",
      "    huber_delta: 0.981062295939513\n"
     ]
    }
   ],
   "source": [
    "# now we can run the experiment\n",
    "storage_name = \"sqlite:///diamond_study.db\"\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"diamond-mlp\", \n",
    "    storage=storage_name, \n",
    "    direction=\"minimize\",\n",
    "    load_if_exists=True  # Crucial for restarting\n",
    ")\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True, catch=(Exception,))\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
